# Ollama_local
This is a simple Python script to interact with the locally hosted Ollama API (chat endpoint) using the llama3.2 model. It sends a user prompt and streams the response from the model in real time.
